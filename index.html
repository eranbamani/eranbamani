<!DOCTYPE HTML>
<html>
	<head>
		<title>Eran Bamani Beeri Personal Website</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<!--[if lte IE 8]><script src="css/ie/html5shiv.js"></script><![endif]-->
		<script src="js/jquery.min.js"></script>
		<script src="js/jquery.scrollzer.min.js"></script>
		<script src="js/jquery.scrolly.min.js"></script>
		<script src="js/skel.min.js"></script>
		<script src="js/skel-layers.min.js"></script>
		<script src="js/init.js"></script>
		<script src="js/carousel.js"></script>
		<noscript>
			<link rel="stylesheet" href="css/carousel.css" />
			<link rel="stylesheet" href="css/skel.css" />
			<link rel="stylesheet" href="css/style.css" />
			<!-- <link rel="stylesheet" href="css/carousel.css"> -->
			<link rel="stylesheet" href="css/style-xlarge.css" />
		</noscript>
<style type="text/css">
    .carousel {
        -webkit-transform: translate3d(0,0,0);
        background: rgba(0,0,0,0.85);
        position: fixed;
        right: 0;
        bottom: 0;
        min-width: 100%;
        min-height: 100%;
        width: auto;
        height: auto;
        display: none;
        z-index: 1;
    }

    .img-center-carousel {
        position: absolute;
        top: 0;
        left: 0;
        right: 0;
        bottom: 0;
        padding: 0;
        margin: auto;
        width: 60%;
        height: auto;
    }

    .carousel-close {
        position: absolute;
        top: 25px;
        right: 100px;
        padding: 0;
        margin: auto;
        width: 50px;
        height: auto;
    }
    .research {
        margin-bottom: 30px;
    }
    .research h4{
        float: left;
    }
    .research div{
        text-align: end;
        font-size:0.9em;
    }
    .thumbnail {
        width: 30%;
        max-height: 200px;
        float: left;
    }
    .thumbnail-right {
        margin-left: 35%;
    }
    #exp li {
        margin-bottom: 50px;
    }
    .school-logo {
        width: 10%;
        float: left;
    }
    .school-text {
        margin-left: 20%;
        width:60%;
    }
    a {
        color: black;
    }
    b {
        color: blue;
    }

    /* === MIT Style Update === */
    #header {
        background: #2D3C45 !important; /* אפור-כחול מקצועי */
        color: #fff !important;
    }
    #header a, #header h1, #header .icons {
        color: #fff !important;
    }
    #nav ul li a.active, #nav ul li a:hover {
        color: #A31F34 !important; /* אדום MIT */
        font-weight: bold;
    }
    body {
        background: #F7F7F7 !important; /* אפור בהיר רקע */
    }

    /* --- MIT Red Lines (Menu borders + section borders) --- */
    #header > nav ul li {
        border-top: solid 2px #A31F34 !important;
    }
    #main > section {
        border-top: solid 2px #A31F34 !important;
    }

    /* --- MIT Red Skills Bullets --- */
    ul.feature-icons li:before {
        background: #A31F34 !important;
    }
</style>
		<!--[if lte IE 8]><link rel="stylesheet" href="css/ie/v8.css" /><![endif]-->
	</head>
	<body>
		<div id="wrapper">

			<!-- Header -->
				<section id="header" class="skel-layers-fixed">
					<header>
						<span class="image avatar"><img src="images/ERAN.jpeg" alt="" /></span>
						<h1 id="logo"><a href="#">Eran Bamani Beeri</a></h1>
						<a><a href="mailto:eranbamani@gmail.com">eranbamani at gmail dot com</a></a>
						<a><a href = "dated-CV/Postdoctoral Associate CV - Eran Bamani.pdf">curriculum vitae</a></a></a>
					</header>
					<nav id="nav">
						<ul>
							<li><a href="#one" class="active">About Me</a></li>
							<li><a href="#two">Research</a><li>
							<li><a href="#three">Academic Appointments</a></li>
							<li><a href="#four">Education</a></li>
							<li><a href="#five">Awards</a></li>
							<li><a href="#six">Skills</a></li>

						</ul>
					</nav>
					<footer>
						<ul class="icons">
							<!--<li><a href="#" class="icon fa-facebook"><span class="label">Facebook</span></a></li>
							<li><a href="#" class="icon fa-instagram"><span class="label">Instagram</span></a></li>-->
							<li><a href="https://www.linkedin.com/in/eran-beeri-bamani-446503124/" class="icon fa-linkedin"><span class="label">Github</span></a></li>
							<li><a href="mailto:eranbamani@mail.tau.ac.il" class="icon fa-envelope"><span class="label">Email eranbamani@mail.tau.ac.il</span></a></li>
							<li><a href="https://github.com/eranbTAU" class="icon fa-github"><span class="label">Github</span></a></li>
							<li><a href="https://scholar.google.com/citations?hl=en&user=l0y02JwAAAAJ&view_op=list_works&citft=1&email_for_op=eranbamani%40gmail.com&gmla=AJ1KiT0myoFWZptBDbj94G1sWMU1thXjeyf2OU6uoobvRNZOIMED25WX1_UugojgyiCKGCpXKBsgdt5gBBhyfb7YVWUpgmQK6uKzubG0tDBhvuxFQofCHpAJoQ_zD5fAh4vy98PfLwaHJ1OkF_XZ5D6_Do4YOKdLIaSLFKnPrWinv-7q9SnvRsXZu3mJ6orA6kCVMwvU6s4swqUiGzQ6N6Eqxuk17A0oMv6AJX9-5c6Wy-k" target="_blank">Google Scholar</a></li>
        						<li><a href="https://www.youtube.com/@EranBeeriBamani" class="icon fa-youtube"><span class="label">YouTube</span></a></li>

						</ul>
					</footer>
				</section>

			<!-- Main -->
				<div id="main">

					<!-- One -->
						<section id="one">
							<div class="container">
								<header class="major">
									<h3>About Me</h3>
								</header>
								<p> I am a Postdoctoral Associate at the Massachusetts Institute of Technology (MIT) in the 77 Lab, collaborating with Hermano Igo Krebs. My research focuses on developing advanced AI-powered mobile platforms for home-based rehabilitation of neurological movement and balance disorders. I hold a Ph.D. in Deep Learning and Robotics from Tel-Aviv University, where I worked in Dr. Avishai Sintov’s Robotics Lab, and earned both my B.Sc. and M.Sc. in Electronic Engineering at Ariel University under Prof. Yosef Pinhasi, where I conducted research in image and signal processing.</p>
								<p> My research interests include Human-Robot Interaction (HRI), Deep Learning, Computer Vision, and Rehabilitation Robotics.</p>
							</div>
						</section>

					<!-- Two -->	
						<section id="two">
							<div class="container">
								<h3>Research</h3>
								<div class="features">


				<article>
										<div>
											<div class="research"> 
												<h4> <text style="color:Navy"> Speech-to-Trajectory: Learning Human-Like Verbal Guidance for Robot Motion </text></h4>
											</div>
											
											<img class="thumbnail" src="images/tranj.gif" data-id="Scaled-carousel" alt="Research Thumbnail">
											<div id="LPS-carousel" class="carousel">
												<img class="img-center-carousel" src="images/close-transparent.png" alt="Close Button">
												<a href="javascript:void(0)">
													<img src="images/close-transparent.png" alt="" data-carousel-id="Scaled-carousel" class="carousel-close">
												</a>
										  	</div>
											<ul class="thumbnail-right">
												
												<li>
													This work presents the Directive Language Model (DLM), a novel speech-to-trajectory framework that enables robots to follow natural verbal guidance and interpret diverse user commands. DLM maps spoken language directly to executable motion trajectories without relying on predefined phrases, using semantic augmentation and diffusion policy-based trajectory generation. The model achieves robust, human-like motion execution and demonstrates superior accuracy and generalization compared to state-of-the-art methods. 
												</li>
												<li>
													<text style="color:black">Eran Bamani</text>, Eden Nissinman and Avishai Sintov
												</li>
												<li> 
												The paper is currently under review at IEEE Robotics and Automation Letters. <a href="https://arxiv.org/abs/2504.05084" target="_blank">Speech-to-Trajectory: Learning Human-Like Verbal Guidance for Robot Motion </a>. 
 
												</li> 

											</ul>
										</div>
				</article>
									

				<article>
										<div>
											<div class="research"> 
												<h4> <text style="color:Navy"> DiG-Net: Enhancing Quality of Life through Hyper-Range Dynamic Gesture Recognition in Assistive Robotics </text></h4>
											</div>
											
											<img class="thumbnail" src="images/sketch.png" data-id="Scaled-carousel" alt="Research Thumbnail">
											<div id="LPS-carousel" class="carousel">
												<img class="img-center-carousel" src="images/close-transparent.png" alt="Close Button">
												<a href="javascript:void(0)">
													<img src="images/close-transparent.png" alt="" data-carousel-id="Scaled-carousel" class="carousel-close">
												</a>
										  	</div>
											<ul class="thumbnail-right">
												
												<li>
													This work presents DiG-Net, a novel model for assistive robotics that enables accurate recognition of dynamic hand gestures at distances up to 30 meters using only an RGB camera. Integrating depth-conditioned alignment and spatio-temporal graph modules, DiG-Net achieves 97.3% accuracy and significantly improves the real-world usability of assistive robots.
												</li>
												<li>
													<text style="color:black">Eran Bamani</text>, Eden Nissinman and Avishai Sintov
												</li>
												<li> 
													The paper is currently under review at the journal Computer Vision and Image Understanding. 												
												</li> 

											</ul>
										</div>
				</article>
									
									
				<article>
										<div>
											<div class="research"> 
												<h4> <text style="color:Navy"> Recognition of Dynamic Hand Gestures in Long Distance using a Web-Camera for Robot Guidance </text></h4>
											</div>
											
											<img class="thumbnail" src="images/avoid.gif" data-id="Scaled-carousel" alt="Research Thumbnail">
											<div id="LPS-carousel" class="carousel">
												<img class="img-center-carousel" src="images/close-transparent.png" alt="Close Button">
												<a href="javascript:void(0)">
													<img src="images/close-transparent.png" alt="" data-carousel-id="Scaled-carousel" class="carousel-close">
												</a>
										  	</div>
											<ul class="thumbnail-right">
												
												<li>
													In this paper, we propose a model for recognizing dynamic gestures from a long distance of up to 20 meters. The model integrates the SlowFast and Transformer architectures (SFT) to effectively process and classify complex gestures se- sequences captured in video frames. SFT demonstrates superior performance over existing models.
												</li>
												<li>
													<text style="color:black">Eran Bamani</text>, Eden Nissinman and Avishai Sintov
												</li>
												<li> 
													The Paper was accepted to the International Conference on Robotics and Automation 40. <a href="https://arxiv.org/abs/2406.12424" target="_blank">Recognition of Dynamic Hand Gestures in Long Distance using a Web-Camera for Robot Guidance </a>. 
												
												</li> 

											</ul>
										</div>
									</article>
									
									<article>
										<div>
											<div class="research"> 
												<h4> <text style="color:Navy">A Diffusion-based Data Generator for Training Object Recognition Models in Ultra-Range Distance </text></h4>
											</div>
											
											<img class="thumbnail" src="images/synsimg.gif" data-id="Scaled-carousel" alt="Research Thumbnail">
											<div id="LPS-carousel" class="carousel">
												<img class="img-center-carousel" src="images/close-transparent.png" alt="Close Button">
												<a href="javascript:void(0)">
													<img src="images/close-transparent.png" alt="" data-carousel-id="Scaled-carousel" class="carousel-close">
												</a>
										  	</div>
											<ul class="thumbnail-right">
												
												<li>
													In this paper, we propose the Diffusion in Ultra-Range (DUR) framework, utilizing a Diffusion model to generate labeled images of distant objects in various scenes. DUR trains a URGR model on directive gestures, showing superior fidelity and recognition rates compared to other models. Importantly, training a DUR model on limited real data and using it to generate synthetic data for URGR training outperforms direct real data training. We demonstrate its effectiveness in guiding a ground robot with gesture commands.
												</li>
												<li>
													<text style="color:black">Eran Bamani</text>, Eden Nissinman, Inbar Meir, Lisa Koenigsberg and Avishai Sintov
												</li>
												<li> 
													The paper was accepted for publication in IEEE Robotics and Automation Letters. <a href="https://arxiv.org/abs/2404.09846" target="_blank">A Diffusion-based Data Generator for Training Object Recognition Models in Ultra-Range Distance </a>
												
												</li> 

											</ul>
										</div>
									</article>
									
										<article>
										<div>
											<div class="research"> 
												<h4> <text style="color:Navy">Ultra-Range Gesture Recognition using a Web-Camera in Human-Robot Interaction </text></h4>
											</div>
											
											<img class="thumbnail" src="images/scheme6.gif" data-id="Scaled-carousel" alt="Research Thumbnail">
											<div id="LPS-carousel" class="carousel">
												<img class="img-center-carousel" src="images/close-transparent.png" alt="Close Button">
												<a href="javascript:void(0)">
													<img src="images/close-transparent.png" alt="" data-carousel-id="Scaled-carousel" class="carousel-close">
												</a>
										  	</div>
											<ul class="thumbnail-right">
												
												<li>
													In this paper, we address and explore the Ultra-RangeGesture Recognition (URGR) problem and aim for an effective distance of up to 25 meters. We propose a data-based approach that does not require depth information but solely a simple RGB camera. A pioneering aspect of our research involved the development of novel deep-learning architectures. Specifically, HQ-Net was designed to enhance image quality, while GVIT was tailored for the recognition of human gestures.
												</li>
												<li>
													<text style="color:black">Eran Bamani</text>, Eden Nissinman, Inbar Meir, Lisa Koenigsberg and Avishai Sintov
												</li>
												<li> 
													The Paper was accepted for Elsevier ScienceDirect Engineering Applications of Artificial Intelligence 2024. <a href="https://www.sciencedirect.com/science/article/pii/S0952197624006018?dgcid=author" target="_blank">Ultra-Range Gesture Recognition using a Web-Camera in Human-Robot Interaction </a>
												
												</li> 

											</ul>
										</div>
									</article>
									
										<article>
										<div>
											<div class="research"> 
												<h4> <text style="color:Navy">Flip-U-Net for In-Hand Object Recognition Using a Force-Myography Device </text></h4>
											</div>
											
											<img class="thumbnail" src="images/scheme5.gif" data-id="Scaled-carousel" alt="Research Thumbnail">
											<div id="LPS-carousel" class="carousel">
												<img class="img-center-carousel" src="images/close-transparent.png" alt="Close Button">
												<a href="javascript:void(0)">
													<img src="images/close-transparent.png" alt="" data-carousel-id="Scaled-carousel" class="carousel-close">
												</a>
										  	</div>
											<ul class="thumbnail-right">
												
												<li>
													In this paper, we propose a novel Deep Neural-Network architecture for in-hand Object recognition using a wearable Force-Myography Device. The device is based on Force-Myography (FMG) where simple and affordable force sensors measure perturbations of forearm muscles. we show that the proposed network can classify objects grasped by multiple new users without additional training efforts
												</li>
												<li>
													<text style="color:black">Eran Bamani</text>, Nadav Kahanowich, Inbar Ben-David and Avishai Sintov
												</li>
												<li> 
													The Paper was accepted to the International Conference on Robotics and Automation and the Israeli Conference on Robotics.  <a href="Bamani_ICR2023.pdf">In-Hand Object Recognition Using a Force-Myography Device</a></a>
												
												</li> 

											</ul>
										</div>
									</article>
									
										<article>
										<div>
											<div class="research"> 
												<h4> <text style="color:Navy">Recognition and Estimation of Human Finger Pointing with an RGB Camera for Robot Directive </text></h4>
											</div>
											
											<img class="thumbnail" src="images/scheme4.gif" data-id="Scaled-carousel" alt="Research Thumbnail">
											<div id="LPS-carousel" class="carousel">
												<img class="img-center-carousel" src="images/close-transparent.png" alt="Close Button">
												<a href="javascript:void(0)">
													<img src="images/close-transparent.png" alt="" data-carousel-id="Scaled-carousel" class="carousel-close">
												</a>
										  	</div>
											<ul class="thumbnail-right">
												
												<li>
													In this paper, we explore the learning of models for robots to understand pointing directives in various indoor and outdoor environments solely based on a single RGB camera. A novel framework is proposed which includes a designated model termed PointingNet. PointingNet recognizes the occurrence of pointing followed by approximating the position and direction of the index finger.
												</li>
												<li>
													<text style="color:black">Eran Bamani</text>, Eden Nissinman, Lisa Koenigsberg, Inbar Meir, Yoav Matalon and Avishai Sintov
												</li>
												<li> 
													The Paper is under review. <a href="https://arxiv.org/abs/2307.02949" target="_blank">Recognition and Estimation of Human Finger Pointing with an RGB Camera for Robot Directive </a>
												</li>
											</ul>
										</div>
									</article>
									
										<article>
										<div>
											<div class="research"> 
												<h4> <text style="color:Navy">Open-Sourcing Generative Models for Data-driven In advance Robot Simulations </text></h4>
											</div>
											
											<img class="thumbnail" src="images/schame3.gif" data-id="Scaled-carousel" alt="Research Thumbnail">
											<div id="LPS-carousel" class="carousel">
												<img class="img-center-carousel" src="images/close-transparent.png" alt="Close Button">
												<a href="javascript:void(0)">
													<img src="images/close-transparent.png" alt="" data-carousel-id="Scaled-carousel" class="carousel-close">
												</a>
										  	</div>
											<ul class="thumbnail-right">
												
												<li>
													In this paper, we propose to disseminate a generative model rather than actual recorded data. We propose to use a limited amount of real data on a robot to train a Generative Adversarial Network (GAN). We show on two robotic systems that training a regression model using generated synthetic data provides transition accuracy at least as good as real data. Such model could be open-sourced along with the hardware to provide easy and rapid access to research platforms.
												</li>
												<li>
													<text style="color:black">Eran Bamani</text>, Anton Gurevich, Osher Azulay and Avishai Sintov
												</li>
												<li> 
													The Paper was accepted for NeurIPS Data-centric AI 2021. <a href="NeurIPS2021_Data_centric_AI.pdf">Open-Sourcing Generative Models for Data-driven Robot Simulations, <a href="https://neurips.cc/virtual/2021/workshop/38185" target="_blank">Oral</a></a>
												</li>
											</ul>
										</div>
									</article>
									
																											<article>
										<div>
											<div class="research"> 
												<h4> <text style="color:Navy">Learning a Data-Efficient Model for a Single Agent in Homogeneous Multi-Agent Systems</text>  </h4>
											</div>
											<img class="thumbnail" src="images/scheme2.gif" data-id="Scaled-carousel"></img>
											<div id="LPS-carousel" class="carousel">
												<img class="img-center-carousel" src="images/close-transparent.png" alt="">
												<a href="javascript:void(0)"><img src="images/close-transparent.png" alt="" data-carousel-id="Scaled-carousel" class="carousel-close"></a>
										  	</div>
											<ul class="thumbnail-right">
												<li>In this paper, we present a novel real-to-sim-to-real framework to bridge the reality gap for homogeneous multi-agent systems. First, we propose a novel deep neural-network architecture termed Convolutional-Recurrent Network (CR-Net) to simulate agents.</li>
												<li> <text style="color:black">Eran Bamani</text>, Anton Gurevich,and Avishai Sintov </li>
												<li> The Paper was accepted for Springer Neural Computing and Applications 2023. <a href="https://link.springer.com/article/10.1007/s00521-023-08838-w" target="_blank">Learning a Data-Efficient Model for a Single Agent in Homogeneous Multi-Agent Systems</a></li>
											</ul>
										</div>
									</article>
								
																		<article>
										<div>
											<div class="research"> 
												<h4> <text style="color:Navy">Robust Multi-User In-Hand Object Recognition in Human-Robot Collaboration Using a Wearable Force-Myography Device</text>  </h4>
											</div>
											<img class="thumbnail" src="images/scheme.png" data-id="Scaled-carousel"></img>
											<div id="LPS-carousel" class="carousel">
												<img class="img-center-carousel" src="images/close-transparent.png" alt="">
												<a href="javascript:void(0)"><img src="images/close-transparent.png" alt="" data-carousel-id="Scaled-carousel" class="carousel-close"></a>
										  	</div>
											<ul class="thumbnail-right">
												<li>In this paper, we explore the use of a wearable device to non-visually recognize objects within the human hand in various possible grasps. The device is based on Force-Myography (FMG) where simple and affordable force sensors measure perturbations of forearm muscles. We propose a novel Deep Neural-Network architecture termed Flip-U-Net inspired by the familiar U-Net architecture Used for image segmentation. </li>
												<li> <text style="color:black">Eran Bamani</text>, Nadav D. Kahanowich, Inbar Ben-David and Avishai Sintov </li>
												<li> The Paper was accepted for IEEE robotics and automation letters 2021. <a href="https://ieeexplore.ieee.org/document/9562277" target="_blank">Robust Multi-User In-Hand Object Recognition in Human-Robot Collaboration Using a Wearable Force-Myography</a></li>
											</ul>
										</div>
									</article>
								
								
									<article>
										<div>
											<div class="research">
												<h4> <text style="color:Navy">Scaled Modeling and Measurement for Studying Radio Wave Propagation in Tunnels</text>  </h4>
											</div>
											<img class="thumbnail" src="images/Scaled.png" data-id="Scaled-carousel"></img>
											<div id="LPS-carousel" class="carousel">
												<img class="img-center-carousel" src="images/close-transparent.png" alt="">
												<a href="javascript:void(0)"><img src="images/close-transparent.png" alt="" data-carousel-id="Scaled-carousel" class="carousel-close"></a>
										  	</div>
											<ul class="thumbnail-right">
												<li> This work is based on the ray-tracing approach, which is useful for structures where the dimensions are orders of magnitude larger than the transmission wavelength. Using image theory, we utilized a multi-ray model to reveal non-dimensional parameters, enabling measurements in down-scaled experiments. </li>
												<li> Jacob Gerasimov, Nezah Balal, <text style="color:black">Eran Bamani</text>, Gad A. Pinhasi and Yosef Pinhasi </li>
												<li> The Paper was accepted for MDPI Electronics 2020. <a href="https://www.mdpi.com/2079-9292/10/1/53" target="_blank">Scaled Modeling and Measurement for Studying Radio Wave Propagation in Tunnels </a></li>

											</ul>
										</div>
									</article>

									<article>
										<div>
											<div class="research">
												<h4> <text style="color:Navy">Study Of Human Body Effect On Wireless Indoor Communication</text> </h4>
												<br>
											</div>
											<img class="thumbnail" src="images/Study.png" data-id="Study-carousel"></img>
											<div id="Study-carousel" class="carousel">
												<img class="img-center-carousel" src="images/Study.png" alt="">
												
												<a href="javascript:void(0)"><img src="images/close-transparent.png" alt="" data-carousel-id="Study-carousel" class="carousel-close"></a>
										  	</div>
											<ul class="thumbnail-right">
												<li> Our reaearch presents signal strength measurements, analysis, and prediction models for indoors, outdoors and near human body scenarios. The measurements were conducted by using a continuous wave transmitter and receiver antenna pair at 0.5GHz.</li>
												<li> <text style="color:black">Eran Bamani</text> and Gad A. Pinhasi. </li>
												<li> The Paper was accepted for Israeli - Russian Bi-National Workshop 2019, <a href="Study_Of_Human_Body_Effect_On_Wireless_Indoor_Communication_ws2019cd.pdf">STUDY OF HUMAN BODY EFFECT ON WIRELESS INDOOR COMMUNICATION</a></li>
											</ul>
										</div>
									</article>
								</div>
							</div>
						</section>



					<!-- three -->
						
					
					<section id="three">
							<div class="container">
								<h3>Academic Appointments</h3>
								<ul id="exp" style="list-style:none;">
								<li> <img class="school-logo" src="schools/MIT_logo.png"><div class="school-text">2025 - Present, Massachusetts Institute of Technology (MIT), Cambridge, MA, <br />Postdoctoral Associate at The 77 Lab</div></li>								</ul>
							</div>
						</section>


					<!-- Four -->
						<section id="four">
							<div class="container">
								<h3>Education Background</h3>
								<ul id="exp" style="list-style:none;">
									<li> <img class="school-logo" src="schools/Tel-Aviv.png"><div class="school-text">2021 - 2025, Tel-Aviv University, <br />PhD Student in Deep Learning and Robotics, ISF's Fellow </div></li>
									<br><li> <img class="school-logo" src="schools/Ariel.jpg"><div class="school-text">2013 - 2019, Ariel University, <br />B.Sc. and M.Sc. degree in Electronic Engineering, GPA 92/100 </div></li>
								</ul>
							</div>
						</section>

					<section id="five">
						<div class="container">
							<h3>Awards</h3>
							<ul>
								<li><text style="color:black">Blavatnik Cambridge Fellowship - Selected as one of the Blavatnik Fellows for postdoctoral research at the University of Cambridge </text>, 2025 <br />
								<li><text style="color:black">Outstanding Research Achievement – ME Graduate Research Award (PhD) </text>, 2023 <br />
								<li><text style="color:black">Recognized for outstanding research contributions in HRI by the Israel Innovation Authority (IIA)</text>, 2022 <br />
								<li><text style="color:black">Awarded research excellence recognition by the Israel Science Foundation (ISF)</text>, 2021 <br />
								<li><text style="color:black">Awarded a Ministry of Defense (MAFAT) fellowship for excellence in research</text>, 2017, 2018 <br />
								<li><text style="color:black">Dean's list: second year of B.Sc.</text> 2015 <br />
								<li><text style="color:black">Dean's list and dean’s award (full-tuition scholarship): first year of B.Sc.</text>, 2014 <br />
							</ul>
						</div>
					</section>

					<!-- Five -->
					<section id="six">
							<div class="container">
								<h3>Skills</h3>
								<ul class="feature-icons">
									<li class="fa-code">Deep Learning frameworks: Pytorch, TensorFlow, Keras and Theano</li>
									<li class="fa-code">APIs and Libraries: PyCharm, Spyder, NVidia CUDA, OpenGL, OpenCV</li>
									<li class="fa-book">Programming Languages: Python, C/C++, Java and MATLAB</li>
									<li class="fa-bolt">Experienced with developing new machine learning techniques</li>
									<li class="fa-coffee">Medical CAD: 3D-Slicer and RadiAnt</li>
									<li class="fa-cubes">Robotics: ROS and RVW</li>
								</ul>
							</div>
						</section>

			<!-- Footer -->
				<section id="footer">
					<div class="container">
						<ul class="copyright">
							<li>&copy; Eran Beeri Bamani. All rights reserved.</li>
						</ul>
					</div>
				</section>
			
		</div>
	</body>
</html>
