<!DOCTYPE HTML>
<html>
	<head>
		<title>Eran Bamani Beeri Personal Website</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<!--[if lte IE 8]><script src="css/ie/html5shiv.js"></script><![endif]-->
		<script src="js/jquery.min.js"></script>
		<script src="js/jquery.scrollzer.min.js"></script>
		<script src="js/jquery.scrolly.min.js"></script>
		<script src="js/skel.min.js"></script>
		<script src="js/skel-layers.min.js"></script>
		<script src="js/init.js"></script>
		<script src="js/carousel.js"></script>
		<noscript>
			<link rel="stylesheet" href="css/carousel.css" />
			<link rel="stylesheet" href="css/skel.css" />
			<link rel="stylesheet" href="css/style.css" />
			<!-- <link rel="stylesheet" href="css/carousel.css"> -->
			<link rel="stylesheet" href="css/style-xlarge.css" />
		</noscript>
		<style type="text/css">
			.carousel {
				-webkit-transform: translate3d(0,0,0);
				background: rgba(0,0,0,0.85);
				position: fixed;
				right: 0;
				bottom: 0;
				min-width: 100%;
				min-height: 100%;
				width: auto;
				height: auto;
				display: none;
				z-index: 1;
			}

			.img-center-carousel {
				position: absolute;
				top: 0;
				left: 0;
				right: 0;
				bottom: 0;
				padding: 0;
				margin: auto;
				width: 60%;
				height: auto;
			}

			.carousel-close {
				position: absolute;
				top: 25px;
				right: 100px;
				padding: 0;
				margin: auto;
				width: 50px;
				height: auto;
			}
			.research {
				margin-bottom: 30px;
			}
			.research h4{
				float: left;
			}
			.research div{
				text-align: end;
				font-size:0.9em;
			}
			.thumbnail {
				width: 30%;
				max-height: 200px;
				float: left;
			}
			.thumbnail-right {
				margin-left: 35%;
			}
			#exp li {
				margin-bottom: 50px;
			}
			.school-logo {
				width: 10%;
				float: left;
			}
			.school-text {
				margin-left: 20%;
				width:60%;
			}
			a {
				color: black;
			}

			}
			b {
				color: blue;
			}

		</style>
		<!--[if lte IE 8]><link rel="stylesheet" href="css/ie/v8.css" /><![endif]-->
	</head>
	<body>
		<div id="wrapper">

			<!-- Header -->
				<section id="header" class="skel-layers-fixed">
					<header>
						<span class="image avatar"><img src="images/Eran_avatar.png" alt="" /></span>
						<h1 id="logo"><a href="#">Eran Bamani Beeri</a></h1>
						<a><a href="mailto:eranbamani@gmail.com">eranbamani at gmail dot com</a></a>
						<a><a href = "dated-CV/PhD student CV - Eran Bamani.pdf">curriculum vitae</a></a></a>
					</header>
					<nav id="nav">
						<ul>
							<li><a href="#one" class="active">About Me</a></li>
							<li><a href="#two">Research</a><li>
							<li><a href="#three">Projects</a></li>
							<li><a href="#four">Education</a></li>
							<li><a href="#five">Awards</a></li>
							<li><a href="#six">Skills</a></li>

						</ul>
					</nav>
					<footer>
						<ul class="icons">
							<!--<li><a href="#" class="icon fa-facebook"><span class="label">Facebook</span></a></li>
							<li><a href="#" class="icon fa-instagram"><span class="label">Instagram</span></a></li>-->
							<li><a href="https://www.linkedin.com/in/eran-beeri-bamani-446503124/" class="icon fa-linkedin"><span class="label">Github</span></a></li>
							<li><a href="mailto:eranbamani@mail.tau.ac.il" class="icon fa-envelope"><span class="label">Email eranbamani@mail.tau.ac.il</span></a></li>
							<li><a href="https://github.com/eranbTAU" class="icon fa-github"><span class="label">Github</span></a></li>
							<li><a href="https://scholar.google.com/citations?hl=en&user=l0y02JwAAAAJ&view_op=list_works&citft=1&email_for_op=eranbamani%40gmail.com&gmla=AJ1KiT0myoFWZptBDbj94G1sWMU1thXjeyf2OU6uoobvRNZOIMED25WX1_UugojgyiCKGCpXKBsgdt5gBBhyfb7YVWUpgmQK6uKzubG0tDBhvuxFQofCHpAJoQ_zD5fAh4vy98PfLwaHJ1OkF_XZ5D6_Do4YOKdLIaSLFKnPrWinv-7q9SnvRsXZu3mJ6orA6kCVMwvU6s4swqUiGzQ6N6Eqxuk17A0oMv6AJX9-5c6Wy-k" target="_blank">Google Scholar</a></li>
        						<li><a href="https://www.youtube.com/@EranBeeriBamani" class="icon fa-youtube"><span class="label">YouTube</span></a></li>

						</ul>
					</footer>
				</section>

			<!-- Main -->
				<div id="main">

					<!-- One -->
						<section id="one">
							<div class="container">
								<header class="major">
									<h3>About Me</h3>
								</header>
								<p> I am a PhD student in Deep Learning and Robotics at <a href="https://english.tau.ac.il/"> Tel-Aviv University</a>, working on Deep Learning, Computer Vision, Robotics and Human-Robot Interaction. I work in the Robotics Lab under the supervision of Dr. <a href="https://english.tau.ac.il/profile/sintov1/">Avishai Sintov</a>. Before entering TAU, I worked with Prof. <a href="https://www.cs.huji.ac.il/~werman/">Michael Werman</a> at <a href="https://en.huji.ac.il/en/"> The Hebrew University of Jerusalem</a> on Medical Imaging Processing and Generative Adversarial Networks for Medical Images.  </p>
								<p> I received my B.Sc. and M.Sc. both in Electronic Engineering from <a href="https://www.ariel.ac.il/wp/en/">Ariel University</a> under the supervision of Prof. <a href="ariel.ac.il/wp/yosip/">Yosef Pinhasi</a>. I worked at Homeland Security Laboratory on Ministry of Defense (MAFAT) research. My research field was Image and Signal Processing and Estimation Techniques. </p>
								<p> My research interests include: Deep Learning, Computer Vision, Robotics, Human–Robot Interaction (HRI), and Human-Robot Collaboration (HRC).	</p>
							</div>
						</section>

					<!-- Two -->	
						<section id="two">
							<div class="container">
								<h3>Research</h3>
								<div class="features">

									<article>
										<div>
											<div class="research"> 
												<h4> <text style="color:Navy">A Diffusion-based Data Generator for Training Object Recognition Models in Ultra-Range Distance </text></h4>
											</div>
											
											<img class="thumbnail" src="images/synsimg.gif" data-id="Scaled-carousel" alt="Research Thumbnail">
											<div id="LPS-carousel" class="carousel">
												<img class="img-center-carousel" src="images/close-transparent.png" alt="Close Button">
												<a href="javascript:void(0)">
													<img src="images/close-transparent.png" alt="" data-carousel-id="Scaled-carousel" class="carousel-close">
												</a>
										  	</div>
											<ul class="thumbnail-right">
												
												<li>
													In this paper, we propose the Diffusion in Ultra-Range (DUR) framework, utilizing a Diffusion model to generate labeled images of distant objects in various scenes. DUR trains a URGR model on directive gestures, showing superior fidelity and recognition rates compared to other models. Importantly, training a DUR model on limited real data and using it to generate synthetic data for URGR training outperforms direct real data training. We demonstrate its effectiveness in guiding a ground robot with gesture commands.
												</li>
												<li>
													<text style="color:black">Eran Bamani</text>, Eden Nissinman, Inbar Meir, Lisa Koenigsberg and Avishai Sintov
												</li>
												<li> 
													The Paper is under review. <a href="https://arxiv.org/abs/2404.09846" target="_blank">A Diffusion-based Data Generator for Training Object Recognition Models in Ultra-Range Distance </a>
												
												</li> 

											</ul>
										</div>
									</article>
									
										<article>
										<div>
											<div class="research"> 
												<h4> <text style="color:Navy">Ultra-Range Gesture Recognition using a Web-Camera in Human-Robot Interaction </text></h4>
											</div>
											
											<img class="thumbnail" src="images/scheme6.gif" data-id="Scaled-carousel" alt="Research Thumbnail">
											<div id="LPS-carousel" class="carousel">
												<img class="img-center-carousel" src="images/close-transparent.png" alt="Close Button">
												<a href="javascript:void(0)">
													<img src="images/close-transparent.png" alt="" data-carousel-id="Scaled-carousel" class="carousel-close">
												</a>
										  	</div>
											<ul class="thumbnail-right">
												
												<li>
													In this paper, we address and explore the Ultra-RangeGesture Recognition (URGR) problem and aim for an effective distance of up to 25 meters. We propose a data-based approach that does not require depth information but solely a simple RGB camera. A pioneering aspect of our research involved the development of novel deep-learning architectures. Specifically, HQ-Net was designed to enhance image quality, while GVIT was tailored for the recognition of human gestures.
												</li>
												<li>
													<text style="color:black">Eran Bamani</text>, Eden Nissinman, Inbar Meir, Lisa Koenigsberg and Avishai Sintov
												</li>
												<li> 
													The Paper was accepted for Elsevier ScienceDirect Engineering Applications of Artificial Intelligence 2024. <a href="https://www.sciencedirect.com/science/article/pii/S0952197624006018?dgcid=author" target="_blank">Ultra-Range Gesture Recognition using a Web-Camera in Human-Robot Interaction </a>
												
												</li> 

											</ul>
										</div>
									</article>
									
										<article>
										<div>
											<div class="research"> 
												<h4> <text style="color:Navy">Flip-U-Net for In-Hand Object Recognition Using a Force-Myography Device </text></h4>
											</div>
											
											<img class="thumbnail" src="images/scheme5.gif" data-id="Scaled-carousel" alt="Research Thumbnail">
											<div id="LPS-carousel" class="carousel">
												<img class="img-center-carousel" src="images/close-transparent.png" alt="Close Button">
												<a href="javascript:void(0)">
													<img src="images/close-transparent.png" alt="" data-carousel-id="Scaled-carousel" class="carousel-close">
												</a>
										  	</div>
											<ul class="thumbnail-right">
												
												<li>
													In this paper, we propose a novel Deep Neural-Network architecture for in-hand Object recognition using a wearable Force-Myography Device. The device is based on Force-Myography (FMG) where simple and affordable force sensors measure perturbations of forearm muscles. we show that the proposed network can classify objects grasped by multiple new users without additional training efforts
												</li>
												<li>
													<text style="color:black">Eran Bamani</text>, Nadav Kahanowich, Inbar Ben-David and Avishai Sintov
												</li>
												<li> 
													The Paper was accepted to the International Conference on Robotics and Automation and the Israeli Conference on Robotics.  <a href="Bamani_ICR2023.pdf">In-Hand Object Recognition Using a Force-Myography Device, <a href="https://ras.papercept.net/conferences/conferences/ICRA23/program/ICRA23_ContentListWeb_2.html" target="_blank">Oral</a></a>
												
												</li> 

											</ul>
										</div>
									</article>
									
										<article>
										<div>
											<div class="research"> 
												<h4> <text style="color:Navy">Recognition and Estimation of Human Finger Pointing with an RGB Camera for Robot Directive </text></h4>
											</div>
											
											<img class="thumbnail" src="images/scheme4.gif" data-id="Scaled-carousel" alt="Research Thumbnail">
											<div id="LPS-carousel" class="carousel">
												<img class="img-center-carousel" src="images/close-transparent.png" alt="Close Button">
												<a href="javascript:void(0)">
													<img src="images/close-transparent.png" alt="" data-carousel-id="Scaled-carousel" class="carousel-close">
												</a>
										  	</div>
											<ul class="thumbnail-right">
												
												<li>
													In this paper, we explore the learning of models for robots to understand pointing directives in various indoor and outdoor environments solely based on a single RGB camera. A novel framework is proposed which includes a designated model termed PointingNet. PointingNet recognizes the occurrence of pointing followed by approximating the position and direction of the index finger.
												</li>
												<li>
													<text style="color:black">Eran Bamani</text>, Eden Nissinman, Lisa Koenigsberg, Inbar Meir, Yoav Matalon and Avishai Sintov
												</li>
												<li> 
													The Paper is under review. <a href="https://arxiv.org/abs/2307.02949" target="_blank">Recognition and Estimation of Human Finger Pointing with an RGB Camera for Robot Directive </a>
												</li>
											</ul>
										</div>
									</article>
									
										<article>
										<div>
											<div class="research"> 
												<h4> <text style="color:Navy">Open-Sourcing Generative Models for Data-driven In advance Robot Simulations </text></h4>
											</div>
											
											<img class="thumbnail" src="images/schame3.gif" data-id="Scaled-carousel" alt="Research Thumbnail">
											<div id="LPS-carousel" class="carousel">
												<img class="img-center-carousel" src="images/close-transparent.png" alt="Close Button">
												<a href="javascript:void(0)">
													<img src="images/close-transparent.png" alt="" data-carousel-id="Scaled-carousel" class="carousel-close">
												</a>
										  	</div>
											<ul class="thumbnail-right">
												
												<li>
													In this paper, we propose to disseminate a generative model rather than actual recorded data. We propose to use a limited amount of real data on a robot to train a Generative Adversarial Network (GAN). We show on two robotic systems that training a regression model using generated synthetic data provides transition accuracy at least as good as real data. Such model could be open-sourced along with the hardware to provide easy and rapid access to research platforms.
												</li>
												<li>
													<text style="color:black">Eran Bamani</text>, Anton Gurevich, Osher Azulay and Avishai Sintov
												</li>
												<li> 
													The Paper was accepted for NeurIPS Data-centric AI 2021. <a href="NeurIPS2021_Data_centric_AI.pdf">Open-Sourcing Generative Models for Data-driven Robot Simulations, <a href="https://neurips.cc/virtual/2021/workshop/38185" target="_blank">Oral</a></a>
												</li>
											</ul>
										</div>
									</article>
									
																											<article>
										<div>
											<div class="research"> 
												<h4> <text style="color:Navy">Learning a Data-Efficient Model for a Single Agent in Homogeneous Multi-Agent Systems</text>  </h4>
											</div>
											<img class="thumbnail" src="images/scheme2.gif" data-id="Scaled-carousel"></img>
											<div id="LPS-carousel" class="carousel">
												<img class="img-center-carousel" src="images/close-transparent.png" alt="">
												<a href="javascript:void(0)"><img src="images/close-transparent.png" alt="" data-carousel-id="Scaled-carousel" class="carousel-close"></a>
										  	</div>
											<ul class="thumbnail-right">
												<li>In this paper, we present a novel real-to-sim-to-real framework to bridge the reality gap for homogeneous multi-agent systems. First, we propose a novel deep neural-network architecture termed Convolutional-Recurrent Network (CR-Net) to simulate agents.</li>
												<li> <text style="color:black">Eran Bamani</text>, Anton Gurevich,and Avishai Sintov </li>
												<li> The Paper was accepted for Springer Neural Computing and Applications 2023. <a href="https://link.springer.com/article/10.1007/s00521-023-08838-w" target="_blank">Learning a Data-Efficient Model for a Single Agent in Homogeneous Multi-Agent Systems</a></li>.
											</ul>
										</div>
									</article>
								
																		<article>
										<div>
											<div class="research"> 
												<h4> <text style="color:Navy">Robust Multi-User In-Hand Object Recognition in Human-Robot Collaboration Using a Wearable Force-Myography Device</text>  </h4>
											</div>
											<img class="thumbnail" src="images/scheme.png" data-id="Scaled-carousel"></img>
											<div id="LPS-carousel" class="carousel">
												<img class="img-center-carousel" src="images/close-transparent.png" alt="">
												<a href="javascript:void(0)"><img src="images/close-transparent.png" alt="" data-carousel-id="Scaled-carousel" class="carousel-close"></a>
										  	</div>
											<ul class="thumbnail-right">
												<li>In this paper, we explore the use of a wearable device to non-visually recognize objects within the human hand in various possible grasps. The device is based on Force-Myography (FMG) where simple and affordable force sensors measure perturbations of forearm muscles. We propose a novel Deep Neural-Network architecture termed Flip-U-Net inspired by the familiar U-Net architecture Used for image segmentation. </li>
												<li> <text style="color:black">Eran Bamani</text>, Nadav D. Kahanowich, Inbar Ben-David and Avishai Sintov </li>
												<li> The Paper was accepted for IEEE robotics and automation letters 2021. <a href="https://ieeexplore.ieee.org/document/9562277" target="_blank">Robust Multi-User In-Hand Object Recognition in Human-Robot Collaboration Using a Wearable Force-Myography</a></li>.
											</ul>
										</div>
									</article>
								
								
									<article>
										<div>
											<div class="research">
												<h4> <text style="color:Navy">Scaled Modeling and Measurement for Studying Radio Wave Propagation in Tunnels</text>  </h4>
											</div>
											<img class="thumbnail" src="images/Scaled.png" data-id="Scaled-carousel"></img>
											<div id="LPS-carousel" class="carousel">
												<img class="img-center-carousel" src="images/close-transparent.png" alt="">
												<a href="javascript:void(0)"><img src="images/close-transparent.png" alt="" data-carousel-id="Scaled-carousel" class="carousel-close"></a>
										  	</div>
											<ul class="thumbnail-right">
												<li> This work is based on the ray-tracing approach, which is useful for structures where the dimensions are orders of magnitude larger than the transmission wavelength. Using image theory, we utilized a multi-ray model to reveal non-dimensional parameters, enabling measurements in down-scaled experiments. </li>
												<li> Jacob Gerasimov, Nezah Balal, <text style="color:black">Eran Bamani</text>, Gad A. Pinhasi and Yosef Pinhasi </li>
												<li> The Paper was accepted for MDPI Electronics 2020. <a href="https://www.mdpi.com/2079-9292/10/1/53" target="_blank">Scaled Modeling and Measurement for Studying Radio Wave Propagation in Tunnels </a></li>.

											</ul>
										</div>
									</article>

									<article>
										<div>
											<div class="research">
												<h4> <text style="color:Navy">Study Of Human Body Effect On Wireless Indoor Communication</text> </h4>
												<br>
											</div>
											<img class="thumbnail" src="images/Study.png" data-id="Study-carousel"></img>
											<div id="Study-carousel" class="carousel">
												<img class="img-center-carousel" src="images/Study.png" alt="">
												
												<a href="javascript:void(0)"><img src="images/close-transparent.png" alt="" data-carousel-id="Study-carousel" class="carousel-close"></a>
										  	</div>
											<ul class="thumbnail-right">
												<li> Our reaearch presents signal strength measurements, analysis, and prediction models for indoors, outdoors and near human body scenarios. The measurements were conducted by using a continuous wave transmitter and receiver antenna pair at 0.5GHz.</li>
												<li> <text style="color:black">Eran Bamani</text> and Gad A. Pinhasi. </li>
												<li> The Paper was accepted for Israeli - Russian Bi-National Workshop 2019, <a href="Study_Of_Human_Body_Effect_On_Wireless_Indoor_Communication_ws2019cd.pdf">STUDY OF HUMAN BODY EFFECT ON WIRELESS INDOOR COMMUNICATION</a></li>.
											</ul>
										</div>
									</article>
								</div>
							</div>
						</section>



					<!-- three -->
						
					
					<section id="three">
						<div class="container">
							<h3>Projects</h3>
							<font size="-0.5">
								<ul>
									<li> Drone Detection <br />
										<a href="https://github.com/eranbamani/DronDetection_ML_ALGO/">DronDetection_ML</a>, 2018. </li>
									<li> Face Detection and Recognition <br />
										<a href="https://github.com/eranbamani/Face-detection-and-recognition-with-ML/">Face detection and recognition with ML</a>, 2017. </li>
									<li> Letters frequency with Monte Carlo and Huffman code <br />
										<a href="https://github.com/eranbamani/Letters-frequency-with-Monte-Carlo-and-Huffman-code/">Letters frequency</a>, 2017. </li>
									<li> Skin Detection With a Support Vector Machine (SVM) <br />
										<a href="https://github.com/eranbamani/Skin-Detection-with-SVM-from-Scratch/">Skin Detection with SVM</a>, 2016. </li>
								</ul>
								</font>
						</div>
					</section>


					<!-- Four -->
						<section id="four">
							<div class="container">
								<h3>Education Background</h3>
								<ul id="exp" style="list-style:none;">
									<li> <img class="school-logo" src="schools/Tel-Aviv.png"><div class="school-text">2021 - Present, Tel-Aviv University, <br />PhD Student in Deep Learning and Robotics, ISF's Fellow </div></li>
									<li> <img class="school-logo" src="schools/Hebrew.png"><div class="school-text">2019 - 2020, The Hebrew University of Jerusalem, <br />PhD Student in Deep Learning and Computer Vision </div></li> <br>
									<br><li> <img class="school-logo" src="schools/Ariel.jpg"><div class="school-text">2013 - 2019, Ariel University, <br />B.Sc. and M.Sc. degree in Electronic Engineering, GPA 92/100 </div></li>
								</ul>
							</div>
						</section>

					<section id="five">
						<div class="container">
							<h3>Awards</h3>
							<ul>
								<li><text style="color:black">Outstanding Research Achievement – ME Graduate Research Award (PhD) </text>, 2023 <br />
								<li><text style="color:black">Israel Innovation Authority (IIA) for HRI prize</text>, 2022 <br />
								<li><text style="color:black">Israel Science Foundation (ISF) prize</text>, 2021 <br />
								<li><text style="color:black">Ministry of Defense (MAFAT) prize</text>, 2017, 2018 <br />
								<li><text style="color:black">Dean's Fellowship</text>, 2014, 2015 <br />
							</ul>
						</div>
					</section>

					<!-- Five -->
					<section id="six">
							<div class="container">
								<h3>Skills</h3>
								<ul class="feature-icons">
									<li class="fa-code">Deep Learning frameworks: Pytorch, TensorFlow, Keras and Theano</li>
									<li class="fa-code">APIs and Libraries: PyCharm, Spyder, NVidia CUDA, OpenGL, OpenCV</li>
									<li class="fa-book">Programming Languages: Python, C/C++, Java and MATLAB</li>
									<li class="fa-bolt">Experienced with developing new machine learning techniques</li>
									<li class="fa-coffee">Medical CAD: 3D-Slicer and RadiAnt</li>
									<li class="fa-cubes">Robotics: ROS and RVW</li>
								</ul>
							</div>
						</section>

			<!-- Footer -->
				<section id="footer">
					<div class="container">
						<ul class="copyright">
							<li>&copy; Eran Bamani. All rights reserved.</li>
						</ul>
					</div>
				</section>
			
		</div>
	</body>
</html>
